{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import textdistance  # pip install textdistance, not built-in\n",
    "pd.options.mode.chained_assignment = None\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "# we only take a random sample \n",
    "\n",
    "# paths to df1 (logs) and to df2 (relative popularity)\n",
    "filename_df1 = \"search_history.csv.gz\"\n",
    "filename_df2 = \"query_popularity.csv.gz\"\n",
    "\n",
    "n = 84\n",
    "# it is too slow to load, so for quick check don't use random sample (the one with lambda)\n",
    "df1 = pd.read_csv(filename_df1, compression=\"gzip\", nrows=100   0000)\n",
    "# df1 = pd.read_csv(filename_df1, header=0, skiprows=lambda i: i % n != 0)\n",
    "df2 = pd.read_csv(filename_df2, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get rid of all the fun (remove emoji characters from data)\n",
    "# taken from https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a user appearing too often (might be someone from WB to check results for sanity, but anyway), we remove 'em\n",
    "df1 = df1[df1[\"wbuser_id\"] != \"70311ec9008a31f743c164e6f1198c86\"]\n",
    "\n",
    "\n",
    "# wipe out all the unicode chars\n",
    "df1[\"no_unicode\"] = df1[\"UQ\"].apply(remove_emoji)\n",
    "\n",
    "# remove empty strings\n",
    "df1[\"symbol_len\"] = df1[\"no_unicode\"].apply(lambda x: len(str(x)))\n",
    "df1 = df1[df1[\"symbol_len\"] > 0]\n",
    "\n",
    "# remove too large and too small texts\n",
    "df1 = df1[(df1[\"symbol_len\"] <= 47) & (df1[\"symbol_len\"] >= 4)]  # the logic is, 99% of data lies below symbol_len == 47, and 99% of data lies above symbol_len == 4\n",
    "\n",
    "# all_words to lower case\n",
    "df1[\"no_unicode\"] = df1[\"no_unicode\"].apply(lambda x: str(x).lower())\n",
    "df2[\"query\"] = df2[\"query\"].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build prefix trie and merge df1 and df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging df1 and df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>query</th>\n",
       "      <th>query_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3487</td>\n",
       "      <td>куртка женская осенняя</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2297</td>\n",
       "      <td>ботинки женские</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2131</td>\n",
       "      <td>кроссовки женские</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1449</td>\n",
       "      <td>сумка женская</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1425</td>\n",
       "      <td>платье</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cnt                   query  query_popularity\n",
       "0  3487  куртка женская осенняя                10\n",
       "1  2297         ботинки женские                10\n",
       "2  2131       кроссовки женские                10\n",
       "3  1449           сумка женская                10\n",
       "4  1425                  платье                10"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate direct index \n",
    "\n",
    "direct_table = df1[[\"no_unicode\", \"cnt\"]].groupby(by=\"no_unicode\").count() \n",
    "direct_table[\"no_unicode\"] = direct_table.index\n",
    "direct_table = direct_table.set_index(np.arange(0, direct_table.shape[0]))\n",
    "direct_table = direct_table.sort_values(by=\"cnt\", ascending=False)\n",
    "direct_table = direct_table.set_index(np.arange(0, direct_table.shape[0]))\n",
    "\n",
    "# merge direct_table (direct_index) with df2 \n",
    "# Here, join of df1 and df2 may be implemented in a fuzzy way (e.g., intersection of n_grams/textdistance between texts < t, etc.), but we just use a direct approach for simplicity\n",
    "\n",
    "direct_table = direct_table.rename(columns={\"no_unicode\": \"query\"})\n",
    "df2_new = df2.set_index(\"query\")\n",
    "direct_index = direct_table.join(df2_new, on=\"query\", how=\"inner\") \n",
    "# remove duplicates\n",
    "direct_index = direct_index.drop_duplicates(\"query\")\n",
    "direct_index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating prefix trie and adding spellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not try to use this trie on a large dataset\n",
    "We implmented this trie just to be fair, but to show the results we use pandas' dataframes, although all the logic of interaction with prefix trie is preserved\n",
    "It is done the way it is done, because we didn't have enough time to build the trie (even from a logs' subset of size 1 million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.endOfWord = False\n",
    "\n",
    "def insertWord(root, word):\n",
    "  '''\n",
    "  Loop through characters in a word and keep adding them at a new node, linking them together\n",
    "  If char already in node, pass\n",
    "  Increment the current to the child with the character\n",
    "  After the characters in word are over, mark current as EOW\n",
    "  '''\n",
    "  current = root\n",
    "  letter = \"\"\n",
    "  for char in word:\n",
    "    if char in current.children.keys():\n",
    "      letter += char\n",
    "    else:\n",
    "      current.children[char] = [Node(), None, letter]\n",
    "      letter += char\n",
    "    current = current.children[char][0]\n",
    "  current.endOfWord = True\n",
    "\n",
    "def allWords(prefix, node, results):\n",
    "  '''\n",
    "  Recursively call the loop\n",
    "  Prefix will be prefix + current character\n",
    "  Node will be position of char's child\n",
    "  results are passed by reference to keep storing result\n",
    "\n",
    "  Eventually, when we reach EOW, the prefix will have all the chars from starting and will be the word that we need. We add this word to the result\n",
    "  '''\n",
    "  if type(node) is list:\n",
    "    node = node[0]\n",
    "\n",
    "  if node.endOfWord:\n",
    "    results.append(prefix)\n",
    "    \n",
    "  for char in node.children.keys():\n",
    "    #print char, node, node.children\n",
    "    allWords(prefix + char, node.children[char], results)\n",
    "\n",
    "def getWordsWithPrefix(prefix, node, prefix_result):\n",
    "  '''\n",
    "  We loop through charcters in the prefix along with trie\n",
    "  If mismatch, return\n",
    "  If no mismatch during iteration, we have reached the end of prefix. Now we need to get words from current to end with the previx that we passed. So call allWords with prefix\n",
    "  '''\n",
    "  current = node\n",
    "  for char in prefix:\n",
    "    if char in current.children.keys():\n",
    "      pass\n",
    "    else:\n",
    "      return\n",
    "    current = current.children[char][0]\n",
    "  allWords(prefix, current, prefix_result)\n",
    "\n",
    "\n",
    "def linkWithDirectIndex(root, direct_index_df):\n",
    "  '''\n",
    "  Link prefix trie and direct index, placing ids of texts in each node\n",
    "  '''\n",
    "\n",
    "  for prefix in root.children:\n",
    "    escaped_prefix = \".*\" + re.escape(root.children[prefix][2] + prefix) + \".*\"\n",
    "    ids = np.array(direct_index_df[direct_index_df[\"no_unicode\"].apply(lambda x: re.match(escaped_prefix, x) is not None)].index)\n",
    "    root.children[prefix][1] = ids\n",
    "\n",
    "    linkWithDirectIndex(root.children[prefix][0], direct_index_df)\n",
    "  \n",
    "  return root\n",
    "\n",
    "def get_texts_from_word(node, word):\n",
    "  # find ids in direct index for a given word (prefix)\n",
    "  current = node\n",
    "  pointer = None\n",
    "  for char in word:\n",
    "    if char in current.children.keys():\n",
    "      pointer = current.children[char][1]\n",
    "      current = current.children[char][0]\n",
    "    else:\n",
    "      # TODO: implement speller here\n",
    "      return  pointer\n",
    "  return pointer\n",
    "\n",
    "def get_texts_from_query(node, query):\n",
    "  # for each word (word - entity with no spaces on the left and right) in a searched text, find candidates from direct_index\n",
    "  words = query.split(\" \")\n",
    "  pointers = []\n",
    "  for word in words:\n",
    "    pointers.append(get_texts_from_word(node, word))\n",
    "  return pointers\n",
    "\n",
    "def intersection_pointers(pointers, k=100):\n",
    "  \"\"\"\n",
    "  Return the intersection of ids (pointers are lists containing ids of texts in direct_index, pointers are linked to a prefix)\n",
    "  \"\"\"\n",
    "\n",
    "  resulting_set = set()\n",
    "  for pointer in pointers:\n",
    "    for text_id in pointer:\n",
    "      if text_id not in resulting_set:\n",
    "        resulting_set.add(text_id)\n",
    "      if resulting_set.__len__() > k:\n",
    "        return resulting_set\n",
    "  return resulting_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): \n",
    "    return re.findall(r'[а-я]+|[a-z]+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('fulltext.txt', encoding=\"utf8\").read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюяabcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "################ Test Code \n",
    "\n",
    "def unit_tests():\n",
    "    assert correction('speling') == 'spelling'              # insert\n",
    "    assert correction('korrectud') == 'corrected'           # replace 2\n",
    "    assert correction('bycycle') == 'bicycle'               # replace\n",
    "    assert correction('inconvient') == 'inconvenient'       # insert 2\n",
    "    assert correction('arrainged') == 'arranged'            # delete\n",
    "    assert correction('peotry') =='poetry'                  # transpose\n",
    "    assert correction('peotryy') =='poetry'                 # transpose + delete\n",
    "    assert correction('word') == 'word'                     # known\n",
    "    assert correction('quintessential') == 'quintessential' # unknown\n",
    "    assert words('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
    "    assert correction('человк') == 'человек'\n",
    "    assert correction('мжчина') == 'мужчина'\n",
    "    assert correction('жнская') == 'женская'\n",
    "    assert correction('зимнння') == 'зимняя'\n",
    "    assert correction('споги') == 'сапоги'\n",
    "    assert correction('наушнии') == 'наушники'\n",
    "    assert correction('крассовки') == 'кроссовки'\n",
    "    assert correction('кошылек') == 'кошелек'\n",
    "    return 'unit_tests pass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge relative (from df2) and absolute (from df1, \"cnt\" column) popularity of logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>query</th>\n",
       "      <th>query_popularity</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3487</td>\n",
       "      <td>куртка женская осенняя</td>\n",
       "      <td>10</td>\n",
       "      <td>34870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2297</td>\n",
       "      <td>ботинки женские</td>\n",
       "      <td>10</td>\n",
       "      <td>22970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2131</td>\n",
       "      <td>кроссовки женские</td>\n",
       "      <td>10</td>\n",
       "      <td>21310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1449</td>\n",
       "      <td>сумка женская</td>\n",
       "      <td>10</td>\n",
       "      <td>14490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1425</td>\n",
       "      <td>платье</td>\n",
       "      <td>10</td>\n",
       "      <td>14250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cnt                   query  query_popularity  combined_score\n",
       "0  3487  куртка женская осенняя                10           34870\n",
       "1  2297         ботинки женские                10           22970\n",
       "2  2131       кроссовки женские                10           21310\n",
       "3  1449           сумка женская                10           14490\n",
       "4  1425                  платье                10           14250"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direct_index_merged = direct_index.copy()\n",
    "direct_index_merged[\"combined_score\"] = direct_index_merged[\"cnt\"] * direct_index_merged[\"query_popularity\"]\n",
    "direct_index_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach works just fine, because what we're trying to do is to maximize both:\n",
    "\n",
    "* Probability of our suggest to be what the user is looking for (the best guess - is to find the most frequent text containing our prefix), that is on account of 'cnt'\n",
    "* Revenue of WB, that is on account of 'query_popularity', because it contains info about products' profitability (profitability of those products that were shown to users as a result of a given search 'query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cnt, we don't need it anymore\n",
    "direct_index_merged = direct_index_merged[[\"query\", \"combined_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what comes next is, essentially, what we'll show during pitches - examples of suggests based on a given search \n",
    "Again,\n",
    "* the data used is just a sample from all the data\n",
    "* when we merged df1 and df2, we didn't make it fuzzy (it is too greedy - read as \"slow\"), but further implementation of fuzzy join may improve results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heterogeneous_suggest(current_words, top_30df, discarded, kept):\n",
    "    weak, strong = None, None\n",
    "    if current_words:\n",
    "        for word1 in current_words:\n",
    "            for word2 in current_words:\n",
    "                if word1 != word2:\n",
    "                    if set(word1.split(\" \")).symmetric_difference(set(word2.split(\" \"))).__len__() == 1:\n",
    "                        weight1 = top_30df[top_30df[\"query\"] == word1][\"combined_and_jaro\"].iloc[0]\n",
    "                        weight2 = top_30df[top_30df[\"query\"] == word2][\"combined_and_jaro\"].iloc[0]\n",
    "                        if weight1 > weight2:\n",
    "                            weak, strong = word2, word1\n",
    "                        else:\n",
    "                            weak, strong = word1, word2\n",
    "                        break\n",
    "            if strong is None:\n",
    "                strong = word1\n",
    "            break\n",
    "        index_strong = current_words.index(strong)\n",
    "        kept.append(current_words.pop(index_strong))\n",
    "        if weak is not None:\n",
    "            index_weak = current_words.index(weak)\n",
    "            discarded.append(current_words.pop(index_weak))\n",
    "        \n",
    "        if len(discarded) > 20:\n",
    "            kept.append(discarded.pop(0))\n",
    "            return kept\n",
    "        else:\n",
    "            heterogeneous_suggest(current_words, top_30df, discarded, kept)\n",
    "            return kept\n",
    "    else:\n",
    "        return kept\n",
    "\n",
    "\n",
    "def get_suggest(searched_prefix):\n",
    "    result_local = direct_index_merged[direct_index_merged[\"query\"].apply(lambda x: re.match(\".*\"+ searched_prefix +\".*\", str(x)) is not None)]  # look for a prefix among all the words\n",
    "    query = result_local[\"query\"]  # all the texts containig searched_prefix\n",
    "    results = tuple(map(lambda x: textdistance.jaro_winkler(searched_prefix, x), np.array(query)))  # we use jaro_winkler text distance as an approximation of sort by words (example here - https://habr.com/ru/company/vk/blog/267469/)\n",
    "    # jaro_winkler has all the properties we need as it respects the words' order and is generally rather a stable metric when it comes to legnth of a word (or texts)\n",
    "\n",
    "\n",
    "    result_local[\"jaro_winkler\"] = results\n",
    "    result_local = result_local[result_local[\"jaro_winkler\"] != 1]  # we don't really need the exact word we typed (say, I searched for a \"car\", why would you suggest me \"car\" as it doesn't add any info?)\n",
    "\n",
    "\n",
    "    result = result_local.sort_values(by=[\"jaro_winkler\", \"combined_score\"], ascending=False)\n",
    "    result_local[\"combined_and_jaro\"] = result_local[\"combined_score\"] * result_local[\"jaro_winkler\"].apply(lambda x: 5*x -4)  # here, it is a penalty for our jaro_winkler ditance. \n",
    "    # This function (namely, slope and intercept) is a hyper-parameter, which basically helps us make changes in jaro_winkler more extreme - it improves the quality of results, but again, it is a hyper-parameter\n",
    "\n",
    "    top30df = result_local[[\"query\", \"combined_and_jaro\"]].sort_values(by=\"combined_and_jaro\", ascending=False).head(30)  # we take first 30 rows to further get rid of results which are like: samsung galaxy a34 | samsung a34\n",
    "    # Which is better? we use a pretty straightforward approach. Since our dataframe is already sorted by popularity of texts and by value the search gives to the business, we just need to take the one that maximazes them! (since we combined them into a one number, \\\n",
    "    # we need to leave just one maximal (by \"combined_and_jaro\") suggest!\n",
    "\n",
    "    kept = set(heterogeneous_suggest(list(top30df[\"query\"]), top30df, [], []))\n",
    "    return top30df[top30df[\"query\"].apply(lambda x: x in kept)].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>combined_and_jaro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>куртка мужская</td>\n",
       "      <td>4700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>куртка мужская осенняя утепленная</td>\n",
       "      <td>1221.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>куртка мужская демисезонная</td>\n",
       "      <td>214.814815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>куртка мужская зимняя с капюшоном</td>\n",
       "      <td>190.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>куртка мужская демисезонная с капюшоном</td>\n",
       "      <td>182.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>куртка мужская осенняя с капюшоном</td>\n",
       "      <td>147.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>куртка мужская утепленная</td>\n",
       "      <td>124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>куртка мужская adidas</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>куртка мужская демисезон</td>\n",
       "      <td>54.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3903</th>\n",
       "      <td>куртка мужская из натуральной кожи</td>\n",
       "      <td>52.941176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        query  combined_and_jaro\n",
       "29                             куртка мужская        4700.000000\n",
       "57          куртка мужская осенняя утепленная        1221.212121\n",
       "948               куртка мужская демисезонная         214.814815\n",
       "846         куртка мужская зимняя с капюшоном         190.909091\n",
       "735   куртка мужская демисезонная с капюшоном         182.051282\n",
       "1115       куртка мужская осенняя с капюшоном         147.058824\n",
       "2068                куртка мужская утепленная         124.000000\n",
       "5605                    куртка мужская adidas          66.666667\n",
       "5931                 куртка мужская демисезон          54.166667\n",
       "3903       куртка мужская из натуральной кожи          52.941176"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 1\n",
    "get_suggest(\"куртка муж\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>combined_and_jaro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>iphone 11</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>iphone 12</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12879</th>\n",
       "      <td>iphone 7</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>iphone 6s чехол</td>\n",
       "      <td>38.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12228</th>\n",
       "      <td>iphone 11 чехлы</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12841</th>\n",
       "      <td>iphone x</td>\n",
       "      <td>31.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>iphone 12 pro max</td>\n",
       "      <td>24.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35109</th>\n",
       "      <td>iphone xr</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31615</th>\n",
       "      <td>iphone 11 128</td>\n",
       "      <td>18.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35156</th>\n",
       "      <td>iphone se 2020</td>\n",
       "      <td>12.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query  combined_and_jaro\n",
       "1374           iphone 11         280.000000\n",
       "5030           iphone 12         100.000000\n",
       "12879           iphone 7          42.000000\n",
       "7088     iphone 6s чехол          38.400000\n",
       "12228    iphone 11 чехлы          32.000000\n",
       "12841           iphone x          31.500000\n",
       "12663  iphone 12 pro max          24.705882\n",
       "35109          iphone xr          20.000000\n",
       "31615      iphone 11 128          18.461538\n",
       "35156     iphone se 2020          12.857143"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 2\n",
    "get_suggest(correction(\"pihone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>combined_and_jaro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>шарики на день рождения</td>\n",
       "      <td>70.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11167</th>\n",
       "      <td>шарики воздушные для праздника</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23640</th>\n",
       "      <td>шарики для манежа</td>\n",
       "      <td>10.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14889</th>\n",
       "      <td>шарики воздушные свадебные</td>\n",
       "      <td>9.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29776</th>\n",
       "      <td>шарики розовые</td>\n",
       "      <td>6.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83291</th>\n",
       "      <td>шарики холодное сердце</td>\n",
       "      <td>2.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121850</th>\n",
       "      <td>шарики для сухого бассейна</td>\n",
       "      <td>2.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122242</th>\n",
       "      <td>шарики с днем рождения</td>\n",
       "      <td>2.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83841</th>\n",
       "      <td>шарики буквы</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121863</th>\n",
       "      <td>шарики для настольного тенниса</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 query  combined_and_jaro\n",
       "1799           шарики на день рождения          70.956522\n",
       "11167   шарики воздушные для праздника          16.000000\n",
       "23640                шарики для манежа          10.588235\n",
       "14889       шарики воздушные свадебные           9.692308\n",
       "29776                   шарики розовые           6.857143\n",
       "83291           шарики холодное сердце           2.727273\n",
       "121850      шарики для сухого бассейна           2.307692\n",
       "122242          шарики с днем рождения           2.181818\n",
       "83841                     шарики буквы           2.000000\n",
       "121863  шарики для настольного тенниса           1.600000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 3\n",
    "get_suggest(correction(\"шарррики\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "119b53cae04f01cd2dca7d1f1483d70f1f6a07646e1e5200a1ba7234cf74e845"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}